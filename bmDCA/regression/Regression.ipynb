{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfnQ-FuiBTEf"
      },
      "source": [
        "# Regression  \n",
        "\n",
        "This is code accompanying paper [Protein language models trained on multiple sequence alignments learn phylogenetic relationships](https://doi.org/10.1038/s41467-022-34032-y). It evaluates the relationship between 144 predictors coming from MSA Transformer column attention's and Patristic distance for pair of sequences.  \n",
        "\n",
        "It was run using Google Colab. First step is installing important libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JS840-5DOW6",
        "outputId": "d0c9b59c-36bd-4d91-c5ba-4e4985fd1ba0"
      },
      "outputs": [],
      "source": [
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY_cScOgqCK3",
        "outputId": "97fa8c35-9a99-47ff-9a38-216675779343"
      },
      "outputs": [],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w65pN-egWAJl",
        "outputId": "eeea80f6-8451-47b7-b7e3-d62a8faec06c"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1oPirLsA_gd"
      },
      "outputs": [],
      "source": [
        "# Start with importing libraries\n",
        "import os\n",
        "import pathlib\n",
        "import itertools\n",
        "import string\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import default_rng\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib import cm\n",
        "\n",
        "from patsy import dmatrices\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
        "    import statsmodels.api as sm\n",
        "\n",
        "import esm\n",
        "import torch\n",
        "\n",
        "from Bio import SeqIO\n",
        "from Bio import Phylo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNHhJwwkUcHG"
      },
      "outputs": [],
      "source": [
        "# Plotting settings\n",
        "SMALL_SIZE = 50\n",
        "MEDIUM_SIZE = 60\n",
        "BIGGER_SIZE = 70\n",
        "plt.rcParams.update({\n",
        "    \"text.usetex\": True,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"times\"],\n",
        "    \"font.size\": MEDIUM_SIZE,\n",
        "    \"axes.titlesize\": BIGGER_SIZE,\n",
        "    \"axes.labelsize\": BIGGER_SIZE,\n",
        "    \"figure.titlesize\": BIGGER_SIZE,\n",
        "    \"xtick.labelsize\": MEDIUM_SIZE,\n",
        "    \"ytick.labelsize\": MEDIUM_SIZE,\n",
        "    \"legend.fontsize\": MEDIUM_SIZE,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdkZanh4CXFp"
      },
      "outputs": [],
      "source": [
        "# Before this they put some plotting setting which we will exclude for now\n",
        "# Setting random seed for fixed performance\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoIIiUu0EZ1o"
      },
      "outputs": [],
      "source": [
        "# This is an efficient way to delete lowercase characters and insertion characters from a string\n",
        "deletekeys = dict.fromkeys(string.ascii_lowercase) # Making dictionary where each lowercase ascii letter is key and value is set to None\n",
        "deletekeys[\".\"] = None\n",
        "deletekeys[\"*\"] = None\n",
        "translation = str.maketrans(deletekeys)\n",
        "\n",
        "def remove_insertions(sequence: str) -> str:\n",
        "    \"\"\" Removes any insertions into the sequences. Needed to load aligned sequences in an MSA.\"\"\"\n",
        "    return sequence.translate(translation)\n",
        "\n",
        "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
        "    \"\"\" Reads the first nseq sequences from an MSA file in fasta format, automatically removes insertions.\"\"\"\n",
        "    return [(record.description, remove_insertions(str(record.seq))) for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w96pKK7JE0Zy"
      },
      "outputs": [],
      "source": [
        "pfam_families = [\n",
        "    \"PF00004\",\n",
        "    \"PF00005\",\n",
        "    \"PF00041\",\n",
        "    \"PF00072\",\n",
        "    \"PF00076\",\n",
        "    \"PF00096\",\n",
        "    \"PF00153\",\n",
        "    \"PF00271\",\n",
        "    \"PF00397\",\n",
        "    \"PF00512\",\n",
        "    \"PF00595\",\n",
        "    \"PF01535\",\n",
        "    \"PF02518\",\n",
        "    \"PF07679\",\n",
        "    \"PF13354\"\n",
        "]\n",
        "\n",
        "MAX_DEPTH = 600\n",
        "\n",
        "n_layers = n_heads = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KeGE-SsMpIh"
      },
      "source": [
        "## Create dataset of Hamming distance matrices and averages of MSA Transformer column attetions averages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell is important to specify folders where the data is stored/will be hosted!  \n",
        "\n",
        "If you are running this Notebook for the first time, you will have data only in `MSAS_FOLDER` - one where .fasta files corresponding to the subsampled (either *random* or *subtree* approach) synthetic PFAM families are stored. \n",
        "\n",
        "Other folders: `DISTS_FOLDER`, `HAMM_FOLDER` and `ATTNS_FOLDER` will be created at the specified path if they don't exist to host *Patristic* and *Hamming distances* of synthetic sequences and *MSA Transformer column attentions* that will be obtained in .npy format in the rest of the Notebook.  \n",
        "\n",
        "Additionally, there is parameter `APPROACH` which specifies whether to use *random* or *subtree* subsampled approach sytnethic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiE7H24RLzPe"
      },
      "outputs": [],
      "source": [
        "# Create a folder to host synthetic MSA sequences, Hamming/Patristic distance matrices and averaged column attention matrices as .npy files\n",
        "APPROACH = 'random'\n",
        "MSAS_FOLDER = pathlib.Path(\"/content/drive/MyDrive/data/subsampled_msa\")\n",
        "HAMM_FOLDER = pathlib.Path(\"/content/drive/MyDrive/data/hamming_subtree\")\n",
        "DISTS_FOLDER = pathlib.Path(\"/content/drive/MyDrive/data/distance_matrix\")\n",
        "ATTNS_FOLDER = pathlib.Path(\"/content/drive/MyDrive/data/col_attentions_{APPROACH}\")\n",
        "\n",
        "\n",
        "for folder in [DISTS_FOLDER, ATTNS_FOLDER, HAMM_FOLDER]:\n",
        "  if not folder.exists():\n",
        "    os.mkdir(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DNR1DT2PXRO",
        "outputId": "8728de9c-cdff-4eb1-c10d-0a459b3dd4a0"
      },
      "outputs": [],
      "source": [
        "# Loading MSA Transformer from pretrained data\n",
        "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
        "msa_transformer = msa_transformer.eval()\n",
        "msa_batch_converter = msa_alphabet.get_batch_converter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH8qxzae4kkV"
      },
      "source": [
        "Next piece of code required running on high-RAM CPU (83GB). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkr93EIxPjm5",
        "outputId": "1b4a8c22-2e34-4f15-f464-4a91d8aebedf"
      },
      "outputs": [],
      "source": [
        "for pfam_family in tqdm.tqdm(pfam_families):\n",
        "\n",
        "  dists_path = DISTS_FOLDER / f\"{pfam_family}_{APPROACH}.npy\"\n",
        "  attns_path = ATTNS_FOLDER / f\"{pfam_family}_{APPROACH}_mean_on_cols_symm.npy\"\n",
        "  hamming_path = HAMM_FOLDER / f\"{pfam_family}_{APPROACH}.npy\"\n",
        "\n",
        "  msa_data = [read_msa(MSAS_FOLDER / f\"{pfam_family}_{APPROACH}.fasta\", MAX_DEPTH)]\n",
        "  msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(msa_data)\n",
        "\n",
        "  # How many aligned sequences is there\n",
        "  depth = msa_batch_tokens.shape[1]\n",
        "  with torch.no_grad():\n",
        "    results = msa_transformer(msa_batch_tokens, repr_layers=[12], need_head_weights=True)\n",
        "\n",
        "\n",
        "  # Compute and save averaged and symmetrized column attention matrices\n",
        "  print(results[\"col_attentions\"].shape)\n",
        "  attns_mean_on_cols_symm = results[\"col_attentions\"].cpu().numpy()[0, ...].mean(axis=2)\n",
        "  print(attns_mean_on_cols_symm.shape)\n",
        "  attns_mean_on_cols_symm += attns_mean_on_cols_symm.transpose(0,1,3,2)\n",
        "  np.save(attns_path, attns_mean_on_cols_symm)\n",
        "\n",
        "  # Compute and save Hamming distance matrices\n",
        "  np.save(hamming_path, squareform(pdist(msa_batch_tokens.cpu().numpy()[0, :, 1:], \"hamming\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzuqRyY0yRLs"
      },
      "source": [
        "Explanation of the sizes that we get.\n",
        "So we have 12 attention heads and 12 layers in each attention head. Next number corresponds to the lenght of the sequence, i.e. how many tokens there was. Since this is column attention, we get one matrix 500x500 per each token (500 corresponds to the number of sequences in the MSA, which we set with the sequence depth).  \n",
        "On that tensor, we first do the mean which averages across all the columns (sites in the protein sequence), leaving us with one matrix of size 500x500 per each layer in each head, thus size (12, 12, 500, 500)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O45jEXbdbUcE",
        "outputId": "3dcb0180-f8eb-4993-e349-6316971a032d"
      },
      "outputs": [],
      "source": [
        "# sanity check - sizes\n",
        "for family in pfam_families:\n",
        "    col_attentions = np.load(ATTNS_FOLDER / f\"{family}_{APPROACH}_mean_on_cols_symm.npy\")\n",
        "    print(col_attentions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XiThbWA9fpw"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "def calculate_correlation(hamming_path, patristic_path):\n",
        "    \"\"\" Calculate correlation between distance matrices obtained by Hamming distances and Patristic distance.\"\"\"\n",
        "\n",
        "    # Load distances from the provided paths\n",
        "    hamming_distance = np.load(hamming_path)\n",
        "    patristic_distance = np.load(patristic_path)\n",
        "\n",
        "    # Diagonals are 0, important infromation is only at the upper triangle of the distance matrix\n",
        "    # We need to flatten the data to compare\n",
        "    hamming_flattened = hamming_distance[np.triu_indices_from(hamming_distance, k=1)]\n",
        "    patristic_flattened = patristic_distance[np.triu_indices_from(patristic_distance, k=1)]\n",
        "\n",
        "    # Calculate Pearson and Sperman correlation\n",
        "    pearsoncorr, _ = pearsonr(hamming_flattened, patristic_flattened)\n",
        "    spearmancorr, _ = spearmanr(hamming_flattened, patristic_flattened)\n",
        "\n",
        "    return pearsoncorr, spearmancorr, hamming_flattened, patristic_flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qk68CzbV_Cwz",
        "outputId": "940e1a40-aaab-4b11-fec4-af032c2b9be7"
      },
      "outputs": [],
      "source": [
        "# Obtain Hamming distances for the synthetic MSAs and calculate the Spearman/Pearson correlation coefficient\n",
        "fig, ax = plt.subplots(figsize=(100, 100), nrows = 5, ncols = 3)\n",
        "for pfam_family in pfam_families:\n",
        "    hamming_path = HAMM_FOLDER / f\"{pfam_family}_{APPROACH}.npy\"\n",
        "    patristic_path = DISTS_FOLDER / f\"{pfam_family}_{APPROACH}.npy\"\n",
        "\n",
        "    pearsoncorr, spearmancorr, hamming_flattened, patristic_flattened = calculate_correlation(hamming_path, patristic_path)\n",
        "\n",
        "    # We have 15 families - to obtain coordinates in the plot\n",
        "    i = pfam_families.index(pfam_family) // 3\n",
        "    j = pfam_families.index(pfam_family) % 3\n",
        "    ax[i, j].scatter(hamming_flattened, patristic_flattened)\n",
        "    ax[i, j].set_title(f\"{pfam_family}: Pearson {pearsoncorr:.2f} // Spearman {spearmancorr:.2f}\")\n",
        "    ax[i, j].set_xlabel(\"Hamming distance\")\n",
        "    ax[i, j].set_ylabel(\"Patristic distance\")\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('pearson_spearman_100_random.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-nZY2N8xR9O"
      },
      "source": [
        "## Fit one logistic model per MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe6Mc5zr1ZlI"
      },
      "outputs": [],
      "source": [
        "def create_train_test_sets(attns, dists, normalize_dists=False, train_size=0.7, ensure_same_size=False, zero_attention_diagonal=False):\n",
        "    \"\"\" Attentions assumed averaged across column dimensions, i.e. 4D tensors.\"\"\"\n",
        "\n",
        "    if zero_attention_diagonal:\n",
        "        attns[:, :, np.arange(attns.shape[2]), np.arange(attns.shape[2])] = 0\n",
        "    assert attns.shape[2] == attns.shape[3]\n",
        "    if normalize_dists:\n",
        "        dists = dists.astype((np.float64))\n",
        "        dists /= np.max(dists)\n",
        "    if ensure_same_size:\n",
        "        dists = dists[:attns.shape[2], :attns.shape[2]]\n",
        "    assert len(dists) == attns.shape[2]\n",
        "    depth = len(dists)\n",
        "    n_layers, n_heads, depth, _ = attns.shape\n",
        "\n",
        "    # Train-test split\n",
        "    n_train = int(depth * train_size) # Meaning that 70% of every MSA family goes to the train dataset\n",
        "    train_idxs = rng.choice(depth, size=n_train, replace=False) # Without replacement choose random 70% of MSA depth\n",
        "    # Determins which indexes are selected to be part of the train set\n",
        "    split_mask = np.zeros(depth, dtype=bool)\n",
        "    split_mask[train_idxs] = True\n",
        "\n",
        "    # Just to extract attentions that correspond to the sequences in the training set\n",
        "    attns_train, attns_test = attns[:, :, split_mask, :][:, :, :, split_mask], attns[:, :, ~split_mask, :][:, :, :, ~split_mask]\n",
        "    dists_train, dists_test = dists[split_mask, :][:, split_mask], dists[~split_mask, :][:, ~split_mask]\n",
        "\n",
        "    # Since attention and distance matrices are symmetric, lower triangular part is redundant\n",
        "    # Besides upper traingle it also includes the diagonal\n",
        "    n_rows_train, n_rows_test = attns_train.shape[-1], attns_test.shape[-1]\n",
        "    triu_indices_train = np.triu_indices(n_rows_train)\n",
        "    triu_indices_test = np.triu_indices(n_rows_test)\n",
        "\n",
        "    attns_train = attns_train[..., triu_indices_train[0], triu_indices_train[1]]\n",
        "    attns_test = attns_test[..., triu_indices_test[0], triu_indices_test[1]]\n",
        "    dists_train = dists_train[triu_indices_train]\n",
        "    dists_test = dists_test[triu_indices_test]\n",
        "\n",
        "    attns_train = attns_train.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
        "    attns_test = attns_test.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
        "\n",
        "    return (attns_train, dists_train), (attns_test, dists_test), (n_rows_train, n_rows_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TLMD_jG5USa"
      },
      "outputs": [],
      "source": [
        "def perform_regressions_msawise(approach=APPROACH, normalize_dists=False, ensure_same_size=False, zero_attention_diagonal=False):\n",
        "\n",
        "    regr_results = {}\n",
        "    for pfam_family in tqdm.tqdm(pfam_families):\n",
        "\n",
        "        # Load paths to the specific column attentions and distances\n",
        "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_{approach}.npy\")\n",
        "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_{approach}_mean_on_cols_symm.npy\")\n",
        "\n",
        "        # Load data\n",
        "        ((attns_train, dists_train),\n",
        "         (attns_test, dists_test),\n",
        "         (n_rows_train, n_rows_test)) = create_train_test_sets(attns,\n",
        "                                                               dists,\n",
        "                                                               normalize_dists=normalize_dists,\n",
        "                                                               ensure_same_size=ensure_same_size,\n",
        "                                                               zero_attention_diagonal=zero_attention_diagonal)\n",
        "\n",
        "        df_train = pd.DataFrame(attns_train, columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
        "        df_train[\"dist\"] = dists_train\n",
        "\n",
        "        df_test = pd.DataFrame(attns_test, columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
        "        df_test[\"dist\"] = dists_test\n",
        "\n",
        "        # Carve out the training matrices from the training and testing data frame using the regression formula\n",
        "        formula = \"dist ~ \" + \" + \".join([f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
        "        y_train, x_train = dmatrices(formula, df_train, return_type=\"dataframe\")\n",
        "        y_test, x_test = dmatrices(formula, df_test, return_type=\"dataframe\")\n",
        "\n",
        "        # Fit the model\n",
        "        binom_model = sm.GLM(y_train, x_train, family=sm.families.Binomial(), cov_type=\"H0\")\n",
        "        binom_model_results = binom_model.fit(maxiter=200, tol=1e-9)\n",
        "\n",
        "        # Make predictions\n",
        "        y_train = y_train[\"dist\"].to_numpy()\n",
        "        y_test = y_test[\"dist\"].to_numpy()\n",
        "        y_pred_train = binom_model_results.predict(x_train).to_numpy()\n",
        "        y_pred_test = binom_model_results.predict(x_test).to_numpy()\n",
        "\n",
        "        regr_results[pfam_family] = {\n",
        "            \"bias\": binom_model_results.params[0],\n",
        "            \"coeffs\": binom_model_results.params.to_numpy()[-n_layers * n_heads:].reshape(n_layers, n_heads),\n",
        "            \"y_train\": y_train,\n",
        "            \"y_pred_train\": y_pred_train,\n",
        "            \"y_test\": y_test,\n",
        "            \"y_pred_test\": y_pred_test,\n",
        "            \"depth\": dists.shape[0],\n",
        "            \"n_rows_train\": n_rows_train,\n",
        "            \"n_rows_test\": n_rows_test\n",
        "            }\n",
        "\n",
        "    return regr_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZQGopfKvKc",
        "outputId": "290057db-e275-4ce9-f2ad-3efa81f32f0e"
      },
      "outputs": [],
      "source": [
        "regr_results_hamming_msawise = perform_regressions_msawise(normalize_dists=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe5MRPTaT2fB"
      },
      "source": [
        "## Plot and analyse the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQGSuE9mUjAL"
      },
      "outputs": [],
      "source": [
        "def create_dist_comparison_mat(y, y_pred, n_rows):\n",
        "    assert len(y) == len(y_pred)\n",
        "\n",
        "    comparison_mat = np.zeros((n_rows, n_rows), dtype=np.float32)\n",
        "    ct = 0\n",
        "    for i in range(n_rows):\n",
        "        for j in range(i, n_rows):\n",
        "            # Order is important as we want the diagonal to be a prediction\n",
        "            comparison_mat[i, j] = y[ct]\n",
        "            comparison_mat[j, i] = y_pred[ct]\n",
        "            ct += 1\n",
        "    assert ct == len(y)\n",
        "\n",
        "    return comparison_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjy1GLXHUn6e"
      },
      "outputs": [],
      "source": [
        "# Select only Pfam families with depth >= 200 and length >=50\n",
        "pfam_families_selec = [\"PF00004\", \"PF00271\", \"PF00512\", \"PF02518\"]\n",
        "n_selec = len(pfam_families_selec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3orSP7GzUyuN",
        "outputId": "eef89e03-453b-4f95-be91-23920ffc7ea9"
      },
      "outputs": [],
      "source": [
        "cmap = cm.bwr\n",
        "vpad = 30\n",
        "x_vals_coeffs = np.arange(0, n_heads, 2)\n",
        "y_vals_coeffs = np.arange(0, n_layers, 2)\n",
        "\n",
        "fig, axs = plt.subplots(\n",
        "    figsize=(40, 10 * n_selec),\n",
        "    nrows=n_selec,\n",
        "    ncols=4,\n",
        "    gridspec_kw={\"width_ratios\": [10, 3, 10, 10]},\n",
        "    constrained_layout=True\n",
        ")\n",
        "\n",
        "for i, pfam_family in enumerate(pfam_families_selec):\n",
        "    res = regr_results_hamming_msawise[pfam_family]\n",
        "    for key in res:\n",
        "        exec(f\"{key} = res['{key}']\")\n",
        "\n",
        "    im = axs[i, 0].imshow(coeffs, norm=colors.CenteredNorm(), cmap=cmap)\n",
        "    cbar = fig.colorbar(im, ax=axs[i, 0], fraction=0.05, pad=0.03)\n",
        "    axs[i, 0].set_ylabel(fr\"\\bf {pfam_family}\" + \"\\nLayer\")\n",
        "    axs[i, 0].set_xticks(x_vals_coeffs)\n",
        "    axs[i, 0].set_yticks(y_vals_coeffs)\n",
        "    axs[i, 0].set_xticklabels(list(map(str, x_vals_coeffs + 1)))\n",
        "    axs[i, 0].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
        "\n",
        "    axs[i, 1].plot(np.mean(np.abs(coeffs), axis=1),\n",
        "                   np.arange(n_layers),\n",
        "                   \"-o\",\n",
        "                   markersize=12,\n",
        "                   lw=5)\n",
        "    axs[i, 1].invert_yaxis()\n",
        "    axs[i, 1].set_yticks(y_vals_coeffs)\n",
        "    axs[i, 1].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
        "    axs[i, 1].set_xlim([0, 55])\n",
        "    axs[i, 1].set_ylabel(\"Layer\")\n",
        "\n",
        "    for j, y, y_pred, n_rows in [(2, y_train, y_pred_train, n_rows_train),\n",
        "                                 (3, y_test, y_pred_test, n_rows_test)]:\n",
        "        # 2 is train, 3 is test\n",
        "        hamming_comparison = create_dist_comparison_mat(y, y_pred, n_rows)\n",
        "        axs[i, j].imshow(np.triu(hamming_comparison, k=1) + np.tril(np.full_like(hamming_comparison, fill_value=np.nan)),\n",
        "                         cmap=\"Blues\",\n",
        "                         vmin=0,\n",
        "                         vmax=1)\n",
        "        pos = axs[i, j].imshow(np.tril(hamming_comparison) + np.triu(np.full_like(hamming_comparison, fill_value=np.nan), k=1),\n",
        "                               cmap=\"Greens\",\n",
        "                               vmin=0,\n",
        "                               vmax=1)\n",
        "\n",
        "    axs[i, 2].set_ylabel(\"Sequence\")\n",
        "\n",
        "axs[0, 0].set_title(\"Regression coefficients\", pad=vpad)\n",
        "axs[0, 1].set_title(\"Avg.\\ abs.\\ coeff.\", pad=vpad)\n",
        "axs[0, 2].set_title(\"Training\", pad=vpad)\n",
        "axs[0, 3].set_title(\"Test\", pad=vpad)\n",
        "\n",
        "axs[-1, 0].set_xlabel(\"Head\")\n",
        "axs[-1, 2].set_xlabel(\"Sequence\")\n",
        "axs[-1, 3].set_xlabel(\"Sequence\")\n",
        "\n",
        "plt.savefig('subtree_500_seq_regression.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "dUobTw91U2ih",
        "outputId": "d0c6e25d-d999-49b8-eb7b-4b2eab35a17e"
      },
      "outputs": [],
      "source": [
        "df_regr_results_hamming_msawise = pd.DataFrame()\n",
        "for pfam_family in pfam_families:\n",
        "    res = regr_results_hamming_msawise[pfam_family]\n",
        "    for key in res:\n",
        "        exec(f\"{key} = res['{key}']\")\n",
        "    n_samples_train = len(y_train)\n",
        "    n_samples_test = len(y_test)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"Depth\"] = depth\n",
        "\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"mean (training)\"] = np.mean(y_train)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"mean (test)\"] = np.mean(y_test)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"std (training)\"] = np.std(y_train)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"std (test)\"] = np.std(y_test)\n",
        "\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"RMSE (training)\"] = np.linalg.norm(y_train - y_pred_train) / np.sqrt(n_samples_train)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"RMSE (test)\"] = np.linalg.norm(y_test - y_pred_test) / np.sqrt(n_samples_test)\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"MAE (training)\"] = np.sum(np.abs(y_train - y_pred_train)) / n_samples_train\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"MAE (test)\"] = np.sum(np.abs(y_test - y_pred_test)) / n_samples_test\n",
        "\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"R^2 (test)\"] = 1 - np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
        "    pearson = pearsonr(y_test, y_pred_test)[0]\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"Pearson (test)\"] = pearson\n",
        "    df_regr_results_hamming_msawise.loc[pfam_family, \"Slope (test)\"] = pearson * np.std(y_pred_test) / np.std(y_test)\n",
        "\n",
        "df_regr_results_hamming_msawise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvlAtFjcD1d"
      },
      "source": [
        "## Regression coefficients from different MSAs are highly correlated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi7LrjWNX2oh"
      },
      "outputs": [],
      "source": [
        "pearsons_coeffs = []\n",
        "for i, pfam_family_x in enumerate(pfam_families):\n",
        "    for pfam_family_y in pfam_families[i + 1:]:\n",
        "        x = regr_results_hamming_msawise[pfam_family_x][\"coeffs\"].flatten()\n",
        "        y = regr_results_hamming_msawise[pfam_family_y][\"coeffs\"].flatten()\n",
        "        pearsons_coeffs.append(pearsonr(x, y)[0])\n",
        "pearsons_coeffs = squareform(np.array(pearsons_coeffs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpysy05dcNVj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select only Pfam families with depth >= 100 and length >=30\n",
        "pfam_families_selec = [\"PF00004\", \"PF00153\", \"PF00271\", \"PF00397\", \"PF00512\", \"PF01535\", \"PF02518\"]\n",
        "mask = np.isin(np.array(pfam_families), [pfam_families_selec])\n",
        "pearsons_coeffs_selec = pearsons_coeffs[mask, :][:, mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "NfCaouOLcP-X",
        "outputId": "92a60e5d-e544-4a94-ae48-9410c27b17d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels = [pfam_family for i, pfam_family in enumerate(pfam_families) if mask[i]]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 12),\n",
        "                       constrained_layout=True)\n",
        "im = ax.imshow(np.tril(pearsons_coeffs_selec),\n",
        "               cmap=\"Blues\",\n",
        "               aspect=\"equal\",\n",
        "               vmin=0, vmax=1)\n",
        "ax.set_yticks(np.arange(len(pearsons_coeffs_selec)),\n",
        "              labels=labels)\n",
        "ax.set_xticks(np.arange(len(pearsons_coeffs_selec)),\n",
        "              labels=labels,\n",
        "              rotation=45,\n",
        "              ha=\"right\")\n",
        "fig.colorbar(im, ax=ax, fraction=0.05, pad=0.04, label=\"Pearson correlation\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI8Hg6d2cjwN"
      },
      "source": [
        "## Fit one common logistic model across MSAs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYg513iOcX4y"
      },
      "outputs": [],
      "source": [
        "pfam_families_train = pfam_families[:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqaSifIAcoqC"
      },
      "outputs": [],
      "source": [
        "def perform_regressions_msawise(pfam_families_train, approach=APPROACH, normalize_dists=False):\n",
        "    df = pd.DataFrame(columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)] + [\"dist\"], dtype=np.float64)\n",
        "\n",
        "    for pfam_family in pfam_families_train:\n",
        "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_{approach}.npy\")\n",
        "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_{approach}_mean_on_cols_symm.npy\")\n",
        "\n",
        "        if normalize_dists:\n",
        "            dists = dists.astype((np.float64))\n",
        "            dists /= np.max(dists)\n",
        "\n",
        "        triu_indices = np.triu_indices(attns.shape[-1])\n",
        "        attns = attns[..., triu_indices[0], triu_indices[1]]\n",
        "        dists = dists[triu_indices]\n",
        "        df2 = pd.DataFrame(attns.transpose(2, 0, 1).reshape(-1, n_layers * n_heads),\n",
        "                           columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_layers)])\n",
        "        df2[\"dist\"] = dists\n",
        "        df = pd.concat([df, df2], ignore_index=True)\n",
        "\n",
        "    # Carve out the training matrices from the training and testing data frame using the regression formula\n",
        "    formula = \"dist ~ \" + \" + \".join([f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
        "    y_train, X_train = dmatrices(formula, df, return_type=\"dataframe\")\n",
        "\n",
        "    # Fit the model\n",
        "    binom_model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
        "    binom_model_results = binom_model.fit(maxiter=200, tol=1e-9)\n",
        "\n",
        "    regr_results_hamming_common = {}\n",
        "    for pfam_family in pfam_families:\n",
        "        dists = np.load(DISTS_FOLDER / f\"{pfam_family}_{approach}.npy\")\n",
        "        attns = np.load(ATTNS_FOLDER / f\"{pfam_family}_{approach}_mean_on_cols_symm.npy\")\n",
        "\n",
        "        if normalize_dists:\n",
        "            dists = dists.astype((np.float64))\n",
        "            dists /= np.max(dists)\n",
        "\n",
        "        depth = len(dists)\n",
        "\n",
        "        triu_indices = np.triu_indices(depth)\n",
        "        attns = attns[..., triu_indices[0], triu_indices[1]]\n",
        "        dists = dists[triu_indices]\n",
        "        attns = attns.transpose(2, 0, 1).reshape(-1, n_layers * n_heads)\n",
        "\n",
        "        df = pd.DataFrame(attns,\n",
        "                          columns=[f\"lyr{i}_hd{j}\" for i in range(n_layers) for j in range(n_heads)])\n",
        "        df[\"dist\"] = dists\n",
        "        _, X = dmatrices(formula, df, return_type=\"dataframe\")\n",
        "\n",
        "        y_pred = binom_model_results.predict(X).to_numpy()\n",
        "\n",
        "        regr_results_hamming_common[pfam_family] = {\n",
        "            \"bias\": binom_model_results.params[0],\n",
        "            \"coeffs\": binom_model_results.params.to_numpy()[-n_layers * n_heads:].reshape(n_layers, n_heads),\n",
        "            \"y\": dists,\n",
        "            \"y_pred\": y_pred,\n",
        "            \"depth\": depth,\n",
        "        }\n",
        "\n",
        "    return regr_results_hamming_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqoieLePcsVW",
        "outputId": "d5c5e33c-c94f-40ee-9a66-2e3d72ec3323"
      },
      "outputs": [],
      "source": [
        "regr_results_hamming_common = perform_regressions_msawise(pfam_families_train, normalize_dists=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNHFB0lLM9oY"
      },
      "source": [
        "## Plot and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "lgLEpR6pcyyZ",
        "outputId": "710581fe-dc49-4b2e-c9d0-c6b1653eeac6"
      },
      "outputs": [],
      "source": [
        "cmap = cm.bwr\n",
        "vpad = 30\n",
        "x_vals_coeffs = np.arange(0, n_heads, 2)\n",
        "y_vals_coeffs = np.arange(0, n_layers, 2)\n",
        "fig, axs = plt.subplots(figsize=(43, 10),\n",
        "                        nrows=1,\n",
        "                        ncols=5,\n",
        "                        gridspec_kw={\"width_ratios\": [10, 3, 10, 10, 10]},\n",
        "                        constrained_layout=True)\n",
        "\n",
        "coeffs = regr_results_hamming_common[pfam_families[0]][\"coeffs\"]\n",
        "im = axs[0].imshow(coeffs, norm=colors.CenteredNorm(), cmap=cmap)\n",
        "cbar = fig.colorbar(im, ax=axs[0], fraction=0.05, pad=0.03)\n",
        "axs[0].set_xticks(x_vals_coeffs)\n",
        "axs[0].set_yticks(y_vals_coeffs)\n",
        "axs[0].set_xticklabels(list(map(str, x_vals_coeffs + 1)))\n",
        "axs[0].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
        "\n",
        "axs[1].plot(np.mean(np.abs(coeffs), axis=1),\n",
        "            np.arange(n_layers),\n",
        "            \"-o\",\n",
        "            markersize=12,\n",
        "            lw=5)\n",
        "axs[1].invert_yaxis()\n",
        "axs[1].set_yticks(y_vals_coeffs)\n",
        "axs[1].set_yticklabels(list(map(str, y_vals_coeffs + 1)))\n",
        "axs[1].set_xticks([0, 10, 20])\n",
        "\n",
        "axs[0].set_title(\"Regression coefficients\", pad=vpad)\n",
        "axs[1].set_title(\"Avg.\\ abs.\\ coeff.\", pad=vpad)\n",
        "\n",
        "for i, pfam_family in enumerate(pfam_families[-3:]):\n",
        "    y = regr_results_hamming_common[pfam_family][\"y\"]\n",
        "    y_pred = regr_results_hamming_common[pfam_family][\"y_pred\"]\n",
        "    n_rows = regr_results_hamming_common[pfam_family][\"depth\"]\n",
        "\n",
        "    hamming_comparison = create_dist_comparison_mat(y, y_pred, n_rows)\n",
        "    axs[2 + i].imshow(np.triu(hamming_comparison, k=1) + np.tril(np.full_like(hamming_comparison, fill_value=np.nan)),\n",
        "                      cmap=\"Blues\",\n",
        "                      vmin=0,\n",
        "                      vmax=1)\n",
        "    pos = axs[2 + i].imshow(np.tril(hamming_comparison) + np.triu(np.full_like(hamming_comparison, fill_value=np.nan), k=1),\n",
        "                            cmap=\"Greens\",\n",
        "                            vmin=0,\n",
        "                            vmax=1)\n",
        "\n",
        "    axs[2 + i].set_xlabel(\"Sequence\")\n",
        "\n",
        "    axs[2 + i].set_title(pfam_family, pad=vpad)\n",
        "\n",
        "axs[0].set_ylabel(\"Layer\")\n",
        "axs[0].set_xlabel(\"Head\")\n",
        "axs[1].set_ylabel(\"Layer\")\n",
        "axs[2].set_ylabel(\"Sequence\")\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('all_msas_random.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "EVDT_BnYNei8",
        "outputId": "cc4c6aa5-9642-49f0-a567-b33cf4ea9ec5"
      },
      "outputs": [],
      "source": [
        "df_regr_results_hamming_common = pd.DataFrame()\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(25, 15),\n",
        "                        nrows=3,\n",
        "                        ncols=5,\n",
        "                        sharex=True,\n",
        "                        sharey=True,\n",
        "                        constrained_layout=True)\n",
        "\n",
        "for i, pfam_family in enumerate(pfam_families):\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"Depth\"] = regr_results_hamming_common[pfam_family][\"depth\"]\n",
        "    y = regr_results_hamming_common[pfam_family][\"y\"]\n",
        "    y_pred = regr_results_hamming_common[pfam_family][\"y_pred\"]\n",
        "    n_samples = len(y)\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"RMSE\"] = np.linalg.norm(y - y_pred) / np.sqrt(n_samples)\n",
        "    y_std = np.std(y)\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"Std\"] = y_std\n",
        "    pearson = pearsonr(y, y_pred)[0]\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"Pearson\"] = pearson\n",
        "    slope = pearson * y_std / np.std(y_pred)\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"Slope\"] = slope\n",
        "    df_regr_results_hamming_common.loc[pfam_family, \"R^2\"] = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
        "    intercept = np.mean(y) - slope * np.mean(y_pred)\n",
        "\n",
        "    axs.flat[i].set_title(pfam_family, fontsize=30)\n",
        "    axs.flat[i].scatter(y_pred, y, s=1)\n",
        "    axs.flat[i].axline((0, intercept), slope=slope, linewidth=2, color='r')\n",
        "\n",
        "    plt.setp(axs.flat[i].get_yticklabels(), fontsize=20)\n",
        "    plt.setp(axs.flat[i].get_xticklabels(), fontsize=20)\n",
        "\n",
        "fig.supxlabel(\"Predicted\", fontsize=40)\n",
        "fig.supylabel(\"Actual\", fontsize=40)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
